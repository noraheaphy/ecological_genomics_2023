gbifID, family, genus, taxonRank, scientificName, verbatimScientificName,
coordinateUncertaintyInMeters, year, basisOfRecord, institutionCode,
collectionCode, catalogNumber, recordNumber, recordedBy)
# add AVH data to GBIF data
occurrences$gbifID <- as.character(occurrences$gbifID)
occurrences <- bind_rows(occurrences, avh_cal)
# get rid of records identified only to family level
occurrences <- occurrences %>% filter(taxonRank != "FAMILY" & taxonRank != "family")
# for all records only identified to genus level, record species names as "Genus L."
occurrences$species <- as.factor(occurrences$species)
View(occurrences)
no_sp <- occurrences %>% filter(is.na(species))
View(no_sp)
for (i in 1:nrow(occurrences)) {
if (is.na(occurrences[i,1])) {
occurrences[i,1] <- paste0(occurrences[i,8], " L.")
}
}
str(occurrences)
# for all records only identified to genus level, record species names as "Genus L."
occurrences$species <- as.character(occurrences$species)
for (i in 1:nrow(occurrences)) {
if (is.na(occurrences[i,1])) {
occurrences[i,1] <- paste0(occurrences[i,8], " L.")
}
}
no_sp <- occurrences %>% filter(is.na(species))
no_sp <- occurrences %>% filter(species == "")
for (i in 1:nrow(occurrences)) {
if (occurrences[i,1] == "") {
occurrences[i,1] <- paste0(occurrences[i,8], " L.")
}
}
View(avh_cal)
no_sp <- occurrences %>% filter(species == "")
View(occurrences)
# first make sure columns are all of right type
occurrences$individualCount <- as.numeric(occurrences$individualCount)
occurrences$coordinateUncertaintyInMeters <- as.numeric(occurrences$coordinateUncertaintyInMeters)
occurrences$year <- as.numeric(occurrences$year)
occurrences_clean <- occurrences %>%
filter(!is.na(decimalLongitude)) %>%
filter(!is.na(decimalLatitude)) %>%
filter(coordinateUncertaintyInMeters <= 1000 |
is.na(coordinateUncertaintyInMeters)) %>%
filter(basisOfRecord == "PRESERVED_SPECIMEN") %>%
filter(individualCount > 0 | is.na(individualCount)) %>%
filter(individualCount < 99 | is.na(individualCount))
bad_occ <- setdiff(occurrences, occurrences_clean)
View(bad_occ)
# convert country code from ISO2c to ISO3c (necessary for next step)
# investigate any errors at https://www.iban.com/country-codes
# ZZ = unknown or unspecified country code
occurrences_clean$countryCode <- countrycode(occurrences_clean$countryCode,
origin = "iso2c",
destination = "iso3c",
warn = TRUE, nomatch = NA)
# flag problems
flags <- clean_coordinates(x = occurrences_clean, lon = "decimalLongitude",
lat = "decimalLatitude",
countries = "countryCode",
species = "species",
tests = c("capitals", "centroids", "equal","gbif",
"institutions", "zeros", "countries",
"seas"))
# exclude problematic records after examining them
occurrences_clean <- occurrences_clean[flags$.summary,]
# round lat and lon to 4 decimal places
occurrences_clean$decimalLatitude <- round(occurrences_clean$decimalLatitude, 4)
occurrences_clean$decimalLongitude <- round(occurrences_clean$decimalLongitude, 4)
View(occurrences_clean)
# dedup
occurrences_clean_deduped <- occurrences_clean %>% distinct(species, decimalLongitude,
decimalLatitude, .keep_all = TRUE)
getwd()
setwd("..")
# these will later need to be filtered for rounding coords to a few decimal places, ditching
# duplicates, sampling 1 record per 1 km2, etc.
write.csv(occurrences_clean_deduped, "calandrinia_all_cleaned_3May2022.csv", row.names = F)
rm(list = ls())
getwd()
setwd("parakeelya_chelsa_extract_3May2022/")
# index all files in the directory (this should be a folder containing only the
# files that you want to merge)
file_list <- list.files()
# note: if you run this command more than once, you need to rm(data_merge) or it
# will keep tagging new records onto the end of last run's dataframe
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("data_merge")){
data_merge <- read.csv(file, header = TRUE, sep = "\t")
}
# if the merged dataset does exist, append to it
if (exists("data_merge")){
temp_dataset <- read.csv(file, header = TRUE, sep = "\t") %>%
dplyr::select(RASTERVALU)
data_merge <- cbind(data_merge, temp_dataset)
rm(temp_dataset)
}
}
View(data_merge)
# get rid of a duplicate column (usually the first RASTRERVALU appears twice)
# also remove ArcGIS-generated XCoord and YCoord columns
data_merge <- data_merge[, c(3:5,7:25)]
# clean up column names
newnames <- c("species", "decimalLongitude", "decimalLatitude",
"CHELSA_01", "CHELSA_02", "CHELSA_03", "CHELSA_04", "CHELSA_05", "CHELSA_06",
"CHELSA_07", "CHELSA_08", "CHELSA_09", "CHELSA_10", "CHELSA_11", "CHELSA_12",
"CHELSA_13", "CHELSA_14", "CHELSA_15", "CHELSA_16", "CHELSA_17",
"CHELSA_18", "CHELSA_19")
oldnames <- names(data_merge)
data_merge <- data_merge %>% rename_with(~ newnames[which(oldnames == .x)],
.cols = oldnames)
View(data_merge)
# check NULL rows in ArcGIS to make sure they're all coastal points, likely over
# the edge of the environmental raster
null <- data_merge %>% filter(CHELSA_01 == "NULL")
write.csv(null, "null_points.csv", row.names = F)
# remove NULL rows
data_merge <- data_merge %>% filter(CHELSA_01 != "NULL")
# check NULL rows in ArcGIS to make sure they're all coastal points, likely over
# the edge of the environmental raster
null <- data_merge %>% filter(CHELSA_15 == "NULL")
# put all CHELSA variables as numeric
for (i in 4:22) {
data_merge[,i] <- as.numeric(data_merge[,i])
}
# put temperature variables in units of degrees C
for (i in 4:14) {
data_merge[,i] <- data_merge[,i] * 0.1
}
View(data_merge)
write.csv(data_merge, "points_all.csv", row.names = F)
setwd("..")
write.csv(data_merge, "parakeelya_chelsa_extract_3May2022.csv", row.names = F)
# bring in both data files
chelsa_data <- read.csv("parakeelya_chelsa_extract_3May2022.csv", header = T, sep = ",", na.strings = "")
gbif_data <- read.csv("calandrinia_all_cleaned_3May2022.csv", header = T, sep = ",", na.strings = "")
# make sure species is a factor and lat/lon are numbers in both datasets
chelsa_data$species <- as.factor(chelsa_data$species)
gbif_data$species <- as.factor(gbif_data$species)
# execute left join
joined_dat <- left_join(gbif_data, chelsa_data, by = c("species" = "species",
"decimalLatitude" = "decimalLatitude",
"decimalLongitude" = "decimalLongitude"))
View(joined_dat)
# remove duplicates
joined_dat <- joined_dat %>% distinct(species, decimalLongitude,
decimalLatitude, .keep_all = TRUE)
# check out null values that didn't make it through join
nulls <- joined_dat %>% filter(is.na(CHELSA_15))
View(null)
View(nulls)
6056-2020
# remove points with null values (coastal)
joined_dat <- joined_dat %>% filter(!is.na(CHELSA_01))
write.csv(joined_dat, "parakeelya_gbif_chelsa_merge_3May2022.csv", row.names = F)
# join australian parakeelya and portulaca
port_aus <- read.csv("portulaca_australia_3May2022.txt", header = T, sep = "\t", na.strings = "")
View(port_aus)
port_aus <- port_aus[,c(3:41)]
View(port_aus)
# join australian parakeelya and portulaca
port_aus <- read.csv("portulaca_australia_3May2022.txt", header = T, sep = "\t", na.strings = "")
port_aus <- port_aus[,c(3:40)]
australia <- bind_rows(port_aus, joined_dat)
View(australia)
1971+4036
# check for nulls
nulls <- australia %>% filter(is.na(CHELSA_01))
write.csv("australia_chelsa_3May2022.csv", row.names = F)
write.csv(australia, "australia_chelsa_3May2022.csv", row.names = F)
View(null)
rm(list = ls())
getwd()
setwd("new_aus_layers/")
# index all files in the directory (this should be a folder containing only the
# files that you want to merge)
file_list <- list.files()
# note: if you run this command more than once, you need to rm(data_merge) or it
# will keep tagging new records onto the end of last run's dataframe
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("data_merge")){
data_merge <- read.csv(file, header = TRUE, sep = "\t")
}
# if the merged dataset does exist, append to it
if (exists("data_merge")){
temp_dataset <- read.csv(file, header = TRUE, sep = "\t") %>%
dplyr::select(RASTERVALU)
data_merge <- cbind(data_merge, temp_dataset)
rm(temp_dataset)
}
}
View(data_merge)
View(data_merge)
# get rid of a duplicate column (usually the first RASTRERVALU appears twice)
# also remove ArcGIS-generated XCoord and YCoord columns
data_merge <- data_merge[, c(3:5,7:57)]
# alternatively
newnames <- c("species", "decimalLongitude", "decimalLatitude", "adefi", "adefm", "adefx",
"arid_i", "arid_m", "arid_x", "c4gi", "evapi", "evapm", "evapx", "maxti", "maxtm",
"maxtx", "megagi", "mesogi", "microgi", "minti", "mintm", "mintx", "pwat_i",
"pwat_m", "pwat_x", "radni", "radnm", "radnx", "rhu215_i", "rhu215_m", "rhu215_x",
"rpreci", "rprecx", "rti_i", "rti_x", "rtx_i", "rtx_x", "spls_i", "spls_m",
"spls_x", "srain1", "srain1mp", "srain2", "srain2mp", "trnga", "trngi", "trngm",
"trngx", "wdef_i", "wdef_m", "wdef_x", "wpot_i", "wpot_m", "wpot_x")
oldnames <- names(data_merge)
data_merge <- data_merge %>% rename_with(~ newnames[which(oldnames == .x)],
.cols = oldnames)
View(data_merge)
# check NULL rows in ArcGIS to make sure they're all coastal points, likely over
# the edge of the environmental raster
null <- data_merge %>% filter(adefi == "NULL")
write.csv(data_merge, "points_all.csv", row.names = F)
View(data_merge)
rm(list = ls())
# bring in both data files
chelsa_data <- read.csv("australia_chelsa_3May2022.csv", header = T, sep = ",", na.strings = "")
setwd("..")
# bring in both data files
chelsa_data <- read.csv("australia_chelsa_3May2022.csv", header = T, sep = ",", na.strings = "")
gbif_data <- read.csv("australia_new_layers_all.csv", header = T, sep = ",", na.strings = "")
# make sure species is a factor and lat/lon are numbers in both datasets
chelsa_data$species <- as.factor(chelsa_data$species)
gbif_data$species <- as.factor(gbif_data$species)
str(chelsa_data)
str(gbif_data)
# execute left join
joined_dat <- left_join(gbif_data, chelsa_data, by = c("species" = "species",
"decimalLatitude" = "decimalLatitude",
"decimalLongitude" = "decimalLongitude"))
# remove duplicates
joined_dat <- joined_dat %>% distinct(species, decimalLongitude,
decimalLatitude, .keep_all = TRUE)
# check out null values that didn't make it through join
nulls <- joined_dat %>% filter(is.na(adefi))
View(chelsa_data)
View(joined_dat)
View(nulls)
getwd()
write.csv(joined_dat, "australia_gbif_chelsa_merge_3May2022.csv", row.names = F)
rm(list = ls())
library(tidyverse)
library(maptools)
library(maps)
library(mapdata)
library(dismo)
library(rgeos)
library(rJava)
library(raster)
library(rgdal)
library(sp)
getwd()
setwd("..")
install.packages("sf")
install.packages("terra")
install.packages("spData")
install.packages("spDataLarge", repos = "https://nowosad.r-universe.dev")
library(tidyverse)
library(ggplot2)
library(ape)
library(ggtree)
install.packages("ggtree")
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("ggtree")
library(ggtree)
sessionInfo()
getwd()
setwd("Desktop/01_projects/course_ecological_genomics/ecological_genomics_2023/project/")
?read.txt
# read in ABBA-BABA results
result <- read.csv("result.Observed.txt", header = T, sep = "\t", na.strings = "")
View(result)
result_transrem <- read.csv("result.TransRem.txt", header = T, sep = "\t", na.strings = "")
View(result_transrem)
getwd()
# read in ABBA-BABA results
result <- read.csv("window_result.Observed.txt", header = T, sep = "\t", na.strings = "")
# read in ABBA-BABA results
window_result <- read.csv("window_result.Observed.txt", header = T, sep = "\t", na.strings = "")
overall_result <- read.csv("result.Observed.txt", header = T, sep = "\t", na.strings = "")
rm(result)
View(overall_result)
View(window_result)
getwd9
getwd()
# read in ABBA-BABA results
overall_result <- read.csv("result.Observed.txt", header = T, sep = "\t", na.strings = "")
window_result10k <- read.csv("window_10k_result.Observed.txt", header = T, sep = "\t", na.strings = "")
window_result5k <- read.csv("window_5k_result.Observed.txt", header = T, sep = "\t", na.strings = "")
window_result1k <- read.csv("window_1k_result.Observed.txt", header = T, sep = "\t", na.strings = "")
View(window_result1k)
View(window_result5k)
View(window_result10k)
View(overall_result)
combined <- read.csv("combined_file.txt", header = T, na.strings = "")
View(combined)
combined <- read.csv("combined_file.txt", header = T, sep = "\s", na.strings = "")
combined <- read.csv("combined_file.txt", header = T, sep = "\t", na.strings = "")
View(combined)
combined <- read.csv("combined_file.txt", header = F, sep = "\t", na.strings = "")
combined <- read.csv("combined_file.txt", header = F, sep = "\t", na.strings = "")
View(combined)
sig <- combined %>% filter(V6 <= 0.05)
library(tidyverse)
library(tidyverse)
sig <- combined %>% filter(V6 <= 0.05)
View(sig)
View(window_result10k)
# make column names
column_names <- vec("D", "JK_D", "V_JK_D", "Z", "pvalue", "nABBA", "nBABA", "nBlocks", "H1", "H2", "H3", "H4")
# make column names
column_names <- c("D", "JK_D", "V_JK_D", "Z", "pvalue", "nABBA", "nBABA", "nBlocks", "H1", "H2", "H3", "H4")
# make column names
column_names <- c("contig", "D", "JK_D", "V_JK_D", "Z", "pvalue", "nABBA", "nBABA", "nBlocks", "H1", "H2", "H3", "H4")
colnames(combined) <- column_names
View(combined)
# fix contig names in first column
combined$contig <- str_replace(combined$contig, "_row5.txt", "")
?p.adjust
# correct for multiple testing
combined$p_adjust <- p.adjust(combined$pvalue, method = "BH")
combined$p_adjust_full <- p.adjust(combined$pvalue, method = "BH", n = 33678)
View(combined)
# pull out significant ones
signif <- combined %>% filter(pvalue <= 0.05)
signif_adj <- combined %>% filter(p_adjust <= 0.05)
signif_adj_full <- combined %>% filter(p_adjust_full <= 0.05)
rm(sig)
rm(window_result10k, window_result1k, window_result5k)
rm(overall_result)
View(signif_adj_full)
# pull out significant ones
signif <- combined %>% filter(pvalue <= 0.0005)
signif_adj <- combined %>% filter(p_adjust <= 0.0005)
signif_adj_full <- combined %>% filter(p_adjust_full <= 0.0005)
View(signif)
# pull out significant ones
signif <- combined %>% filter(pvalue <= 0.0005)
# pull out significant ones
signif <- combined %>% filter(pvalue <= 0.05)
# pull out significant ones
signif <- combined %>% filter(pvalue <= 0.005)
# pull out significant ones
signif <- combined %>% filter(pvalue <= 0.0005)
# pull out significant ones
signif <- combined %>% filter(pvalue <= 0.005)
View(combined)
# pull out significant ones
signif <- combined %>% filter(pvalue < 0.005)
# pull out significant ones
signif <- combined %>% filter(pvalue < 0.0005)
rm(list = ls())
# read in ABBA-BABA results
combined <- read.csv("combined_file.txt", header = F, sep = "\t", na.strings = "")
# make column names
column_names <- c("contig", "D", "JK_D", "V_JK_D", "Z", "pvalue", "nABBA", "nBABA", "nBlocks", "H1", "H2", "H3", "H4")
colnames(combined) <- column_names
# fix contig names in first column
combined$contig <- str_replace(combined$contig, "_row5.txt", "")
# correct for multiple testing
combined$p_adjust <- p.adjust(combined$pvalue, method = "BH")
combined$p_adjust_full <- p.adjust(combined$pvalue, method = "BH", n = 33678)
# pull out significant ones
signif <- combined %>% filter(pvalue <= 0.005)
signif_adj <- combined %>% filter(p_adjust <= 0.0005)
signif_adj_full <- combined %>% filter(p_adjust_full <= 0.0005)
# pull out significant ones
signif <- combined %>% filter(pvalue <= 0.0005)
View(signif)
rm(signif)
View(signif_adj)
rm(list = ls())
# read in ABBA-BABA results
combined <- read.csv("combined_file.txt", header = F, sep = "\t", na.strings = "")
# make column names
column_names <- c("contig", "D", "JK_D", "V_JK_D", "Z", "pvalue", "nABBA", "nBABA", "nBlocks", "H1", "H2", "H3", "H4")
colnames(combined) <- column_names
# fix contig names in first column
combined$contig <- str_replace(combined$contig, "_row5.txt", "")
# correct for multiple testing
combined$p_adjust <- p.adjust(combined$pvalue, method = "BH")
# pull out significant ones
signif <- combined %>% filter(p_adjust <= 0.0005)
View(combined)
# read in ABBA-BABA results
combined <- read.csv("combined_file_v2.txt", header = F, sep = "\t", na.strings = "")
getwd()
# read in ABBA-BABA results
combined <- read.csv("combined_file_v2.txt", header = F, sep = "\t", na.strings = "")
# add in column names
column_names <- c("contig", "D", "JK_D", "V_JK_D", "Z", "pvalue", "nABBA", "nBABA", "nBlocks",
"H1", "H2", "H3", "H4")
colnames(combined) <- column_names
View(combined)
# fix contig names in first column
combined$contig <- str_replace(combined$contig, "_row5.txt", "")
# read in ABBA-BABA results
combined <- read.csv("combined_file_v2.txt", header = F, sep = "\t", na.strings = "")
# add in column names
column_names <- c("contig", "D", "JK_D", "V_JK_D", "Z", "pvalue", "nABBA", "nBABA", "nBlocks",
"H1", "H2", "H3", "H4")
colnames(combined) <- column_names
View(combined)
# fix contig names in first column
combined$contig <- str_replace(combined$contig, "_row5.txt", "")
library(tidyverse)
# correct for multiple testing
combined$p_adjust <- p.adjust(combined$pvalue, method = "BH")
# fix contig names in first column
combined$contig <- str_replace(combined$contig, "_row5.txt", "")
# pull out significant ones
signif <- combined %>% filter(p_adjust <= 0.0005)
View(signif)
# save to file
write.csv(signif, "signif_contigs_abba_v2.csv", row.names = F)
write.csv(signif$contig, "signif_contigs_name_only.txt", row.names = F)
# read in ABBA-BABA results
genome_wide <-  read.csv("result.Observed.txt", header = F, sep = "\t", na.strings = "")
View(genome_wide)
# read in ABBA-BABA results
genome_wide <-  read.csv("result.Observed.txt", header = T, sep = "\t", na.strings = "")
View(signif)
## pull in PC loadings significant genes (from original mapping)
pc_signif <- read.csv("RSBS_PC1_outlier_genes.txt", header = F)
View(pc_signif)
View(signif)
overlap <- signif %>% filter(contig %in% pc_signif)
library(tidyverse)
overlap <- signif %>% filter(contig %in% pc_signif)
overlap <- combined %>% filter(contig %in% pc_signif)
View(combined)
# pull in gene_IDs from ABBA-BABA signif
abba_genes <- read.csv("gene_IDs.txt", header = F)
View(abba_genes)
overlap <- abba_genes %>% filter(V1 %in% pc_signif)
View(overlap)
View(abba_genes)
View(pc_signif)
pc_signif <- pc_signif$V1
overlap <- abba_genes %>% filter(V1 %in% pc_signif)
View(overlap)
overlap
head(overlap)
library(RcppCNPy) # for reading python numpy (.npy) files
getwd()
s <- npyLoad("allRS_poly.selection.npy")
s <- npyLoad("RS_poly.selection.npy")
s <- npyLoad("RS_mapped_poly.selection.npy")
# convert test statistic to p-value
pval <- as.data.frame(1 - pchisq(s, 1))
names(pval) = c("p_PC1", "p_PC2")
## read positions
p <- read.table("RS_mapped_poly_mafs.sites", sep = "\t", header = T, stringsAsFactors = T)
dim(p)
p_filtered = p[which(p$kept_sites==1),]
dim(p_filtered)
## make manhattan plot
plot(-log10(pval$p_PC1),
col = p_filtered$chromo,
xlab = "Position",
ylab = "-log10(p-value)",
main = "Selection outliers: pcANGSD e = 1 (K2)")
View(p)
View(pval)
View(p_filtered)
View(p)
View(overlap)
# pull in genes significant from local PCA
local_signif <- read.csv("local_PCA_geneIDs.txt", header = F)
local_signif <- local_signif$V1
overlap <- abba_genes %>% filter(V1 %in% local_signif)
rm(list = ls())
# read in ABBA-BABA results
genome_wide <-  read.csv("result.Observed.txt", header = T, sep = "\t", na.strings = "")
combined <- read.csv("combined_file_v2.txt", header = F, sep = "\t", na.strings = "")
# add in column names
column_names <- c("contig", "D", "JK_D", "V_JK_D", "Z", "pvalue", "nABBA", "nBABA", "nBlocks",
"H1", "H2", "H3", "H4")
colnames(combined) <- column_names
# fix contig names in first column
combined$contig <- str_replace(combined$contig, "_row5.txt", "")
# correct for multiple testing
combined$p_adjust <- p.adjust(combined$pvalue, method = "BH")
# pull out significant ones
signif <- combined %>% filter(p_adjust <= 0.0005)
## pull in PC loadings significant genes (from original mapping)
pc_signif <- read.csv("RSBS_PC1_outlier_genes.txt", header = F)
pc_signif <- pc_signif$V1
# pull in genes significant from local PCA
local_signif <- read.csv("local_PCA_geneIDs.txt", header = F)
local_signif <- local_signif$V1
# pull in gene_IDs from ABBA-BABA signif
abba_genes <- read.csv("gene_IDs.txt", header = F)
overlap_pc_abba <- abba_genes %>% filter(V1 %in% pc_signif)
overlap_pc_local <- abba_genes %>% filter(V1 %in% local_signif)
## pull in PC loadings significant genes (from original mapping)
pc_signif <- read.csv("RSBS_PC1_outlier_genes.txt", header = F)
# pull in genes significant from local PCA
local_signif <- read.csv("local_PCA_geneIDs.txt", header = F)
pc_signif <- pc_signif$V1
## pull in PC loadings significant genes (from original mapping)
pc_signif <- read.csv("RSBS_PC1_outlier_genes.txt", header = F)
local_signif <- local_signif$V1
overlap_pc_local <- pc_signif %>% filter(V1 %in% local_signif)
overlap_abba_local <- abba_genes %>% filter(V1 %in% local_signif)
View(combined)
# read in ABBA-BABA results
genome_wide <-  read.csv("result.Observed.txt", header = T, sep = "\t", na.strings = "")
View(genome_wide)
