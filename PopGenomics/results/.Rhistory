library(tidyverse)
library(CoordinateCleaner) # for post-download cleaning
library(countrycode) # to convert between two and three digit country codes
library(rgbif) # to pull data from GBIF
library(taxize) # to match taxa names to taxon keys
library(purrr)
library(readr)
library(magrittr) # for %T>% pipe
library(bit64) # for viewing certain GBIF columns correctly
getwd*
)
getwd()
setwd("..")
# get the data out of GBIF and onto your computer using download key from meta
(data <- occ_download_get("0261199-210914110416597"))
occurrences <- occ_download_import(data) %>%
dplyr::select(species, decimalLongitude, decimalLatitude, countryCode,
individualCount, gbifID, family, genus, taxonRank,
scientificName, verbatimScientificName,
coordinateUncertaintyInMeters, year, basisOfRecord,
institutionCode, collectionCode, catalogNumber, recordNumber, recordedBy)
setwd("cli")
setwd("clim_tree_taxa")
getwd()
setwd("data/clim_tree_taxa")
avh_cal <- read.csv("calandrinia_AVH_12Apr2022.csv", header = T, sep = ",", na.strings = "")
# select columns to match GBIF data
avh_cal <- avh_cal %>% mutate(gbifID = "Australian Virtual Herbarium") %>%
dplyr::select(species, decimalLongitude, decimalLatitude, countryCode, individualCount,
gbifID, family, genus, taxonRank, scientificName, verbatimScientificName,
coordinateUncertaintyInMeters, year, basisOfRecord, institutionCode,
collectionCode, catalogNumber, recordNumber, recordedBy)
# add AVH data to GBIF data
occurrences$gbifID <- as.character(occurrences$gbifID)
occurrences <- bind_rows(occurrences, avh_cal)
# get rid of records identified only to family level
occurrences <- occurrences %>% filter(taxonRank != "FAMILY" & taxonRank != "family")
# for all records only identified to genus level, record species names as "Genus L."
occurrences$species <- as.factor(occurrences$species)
View(occurrences)
no_sp <- occurrences %>% filter(is.na(species))
View(no_sp)
for (i in 1:nrow(occurrences)) {
if (is.na(occurrences[i,1])) {
occurrences[i,1] <- paste0(occurrences[i,8], " L.")
}
}
str(occurrences)
# for all records only identified to genus level, record species names as "Genus L."
occurrences$species <- as.character(occurrences$species)
for (i in 1:nrow(occurrences)) {
if (is.na(occurrences[i,1])) {
occurrences[i,1] <- paste0(occurrences[i,8], " L.")
}
}
no_sp <- occurrences %>% filter(is.na(species))
no_sp <- occurrences %>% filter(species == "")
for (i in 1:nrow(occurrences)) {
if (occurrences[i,1] == "") {
occurrences[i,1] <- paste0(occurrences[i,8], " L.")
}
}
View(avh_cal)
no_sp <- occurrences %>% filter(species == "")
View(occurrences)
# first make sure columns are all of right type
occurrences$individualCount <- as.numeric(occurrences$individualCount)
occurrences$coordinateUncertaintyInMeters <- as.numeric(occurrences$coordinateUncertaintyInMeters)
occurrences$year <- as.numeric(occurrences$year)
occurrences_clean <- occurrences %>%
filter(!is.na(decimalLongitude)) %>%
filter(!is.na(decimalLatitude)) %>%
filter(coordinateUncertaintyInMeters <= 1000 |
is.na(coordinateUncertaintyInMeters)) %>%
filter(basisOfRecord == "PRESERVED_SPECIMEN") %>%
filter(individualCount > 0 | is.na(individualCount)) %>%
filter(individualCount < 99 | is.na(individualCount))
bad_occ <- setdiff(occurrences, occurrences_clean)
View(bad_occ)
# convert country code from ISO2c to ISO3c (necessary for next step)
# investigate any errors at https://www.iban.com/country-codes
# ZZ = unknown or unspecified country code
occurrences_clean$countryCode <- countrycode(occurrences_clean$countryCode,
origin = "iso2c",
destination = "iso3c",
warn = TRUE, nomatch = NA)
# flag problems
flags <- clean_coordinates(x = occurrences_clean, lon = "decimalLongitude",
lat = "decimalLatitude",
countries = "countryCode",
species = "species",
tests = c("capitals", "centroids", "equal","gbif",
"institutions", "zeros", "countries",
"seas"))
# exclude problematic records after examining them
occurrences_clean <- occurrences_clean[flags$.summary,]
# round lat and lon to 4 decimal places
occurrences_clean$decimalLatitude <- round(occurrences_clean$decimalLatitude, 4)
occurrences_clean$decimalLongitude <- round(occurrences_clean$decimalLongitude, 4)
View(occurrences_clean)
# dedup
occurrences_clean_deduped <- occurrences_clean %>% distinct(species, decimalLongitude,
decimalLatitude, .keep_all = TRUE)
getwd()
setwd("..")
# these will later need to be filtered for rounding coords to a few decimal places, ditching
# duplicates, sampling 1 record per 1 km2, etc.
write.csv(occurrences_clean_deduped, "calandrinia_all_cleaned_3May2022.csv", row.names = F)
rm(list = ls())
getwd()
setwd("parakeelya_chelsa_extract_3May2022/")
# index all files in the directory (this should be a folder containing only the
# files that you want to merge)
file_list <- list.files()
# note: if you run this command more than once, you need to rm(data_merge) or it
# will keep tagging new records onto the end of last run's dataframe
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("data_merge")){
data_merge <- read.csv(file, header = TRUE, sep = "\t")
}
# if the merged dataset does exist, append to it
if (exists("data_merge")){
temp_dataset <- read.csv(file, header = TRUE, sep = "\t") %>%
dplyr::select(RASTERVALU)
data_merge <- cbind(data_merge, temp_dataset)
rm(temp_dataset)
}
}
View(data_merge)
# get rid of a duplicate column (usually the first RASTRERVALU appears twice)
# also remove ArcGIS-generated XCoord and YCoord columns
data_merge <- data_merge[, c(3:5,7:25)]
# clean up column names
newnames <- c("species", "decimalLongitude", "decimalLatitude",
"CHELSA_01", "CHELSA_02", "CHELSA_03", "CHELSA_04", "CHELSA_05", "CHELSA_06",
"CHELSA_07", "CHELSA_08", "CHELSA_09", "CHELSA_10", "CHELSA_11", "CHELSA_12",
"CHELSA_13", "CHELSA_14", "CHELSA_15", "CHELSA_16", "CHELSA_17",
"CHELSA_18", "CHELSA_19")
oldnames <- names(data_merge)
data_merge <- data_merge %>% rename_with(~ newnames[which(oldnames == .x)],
.cols = oldnames)
View(data_merge)
# check NULL rows in ArcGIS to make sure they're all coastal points, likely over
# the edge of the environmental raster
null <- data_merge %>% filter(CHELSA_01 == "NULL")
write.csv(null, "null_points.csv", row.names = F)
# remove NULL rows
data_merge <- data_merge %>% filter(CHELSA_01 != "NULL")
# check NULL rows in ArcGIS to make sure they're all coastal points, likely over
# the edge of the environmental raster
null <- data_merge %>% filter(CHELSA_15 == "NULL")
# put all CHELSA variables as numeric
for (i in 4:22) {
data_merge[,i] <- as.numeric(data_merge[,i])
}
# put temperature variables in units of degrees C
for (i in 4:14) {
data_merge[,i] <- data_merge[,i] * 0.1
}
View(data_merge)
write.csv(data_merge, "points_all.csv", row.names = F)
setwd("..")
write.csv(data_merge, "parakeelya_chelsa_extract_3May2022.csv", row.names = F)
# bring in both data files
chelsa_data <- read.csv("parakeelya_chelsa_extract_3May2022.csv", header = T, sep = ",", na.strings = "")
gbif_data <- read.csv("calandrinia_all_cleaned_3May2022.csv", header = T, sep = ",", na.strings = "")
# make sure species is a factor and lat/lon are numbers in both datasets
chelsa_data$species <- as.factor(chelsa_data$species)
gbif_data$species <- as.factor(gbif_data$species)
# execute left join
joined_dat <- left_join(gbif_data, chelsa_data, by = c("species" = "species",
"decimalLatitude" = "decimalLatitude",
"decimalLongitude" = "decimalLongitude"))
View(joined_dat)
# remove duplicates
joined_dat <- joined_dat %>% distinct(species, decimalLongitude,
decimalLatitude, .keep_all = TRUE)
# check out null values that didn't make it through join
nulls <- joined_dat %>% filter(is.na(CHELSA_15))
View(null)
View(nulls)
6056-2020
# remove points with null values (coastal)
joined_dat <- joined_dat %>% filter(!is.na(CHELSA_01))
write.csv(joined_dat, "parakeelya_gbif_chelsa_merge_3May2022.csv", row.names = F)
# join australian parakeelya and portulaca
port_aus <- read.csv("portulaca_australia_3May2022.txt", header = T, sep = "\t", na.strings = "")
View(port_aus)
port_aus <- port_aus[,c(3:41)]
View(port_aus)
# join australian parakeelya and portulaca
port_aus <- read.csv("portulaca_australia_3May2022.txt", header = T, sep = "\t", na.strings = "")
port_aus <- port_aus[,c(3:40)]
australia <- bind_rows(port_aus, joined_dat)
View(australia)
1971+4036
# check for nulls
nulls <- australia %>% filter(is.na(CHELSA_01))
write.csv("australia_chelsa_3May2022.csv", row.names = F)
write.csv(australia, "australia_chelsa_3May2022.csv", row.names = F)
View(null)
rm(list = ls())
getwd()
setwd("new_aus_layers/")
# index all files in the directory (this should be a folder containing only the
# files that you want to merge)
file_list <- list.files()
# note: if you run this command more than once, you need to rm(data_merge) or it
# will keep tagging new records onto the end of last run's dataframe
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("data_merge")){
data_merge <- read.csv(file, header = TRUE, sep = "\t")
}
# if the merged dataset does exist, append to it
if (exists("data_merge")){
temp_dataset <- read.csv(file, header = TRUE, sep = "\t") %>%
dplyr::select(RASTERVALU)
data_merge <- cbind(data_merge, temp_dataset)
rm(temp_dataset)
}
}
View(data_merge)
View(data_merge)
# get rid of a duplicate column (usually the first RASTRERVALU appears twice)
# also remove ArcGIS-generated XCoord and YCoord columns
data_merge <- data_merge[, c(3:5,7:57)]
# alternatively
newnames <- c("species", "decimalLongitude", "decimalLatitude", "adefi", "adefm", "adefx",
"arid_i", "arid_m", "arid_x", "c4gi", "evapi", "evapm", "evapx", "maxti", "maxtm",
"maxtx", "megagi", "mesogi", "microgi", "minti", "mintm", "mintx", "pwat_i",
"pwat_m", "pwat_x", "radni", "radnm", "radnx", "rhu215_i", "rhu215_m", "rhu215_x",
"rpreci", "rprecx", "rti_i", "rti_x", "rtx_i", "rtx_x", "spls_i", "spls_m",
"spls_x", "srain1", "srain1mp", "srain2", "srain2mp", "trnga", "trngi", "trngm",
"trngx", "wdef_i", "wdef_m", "wdef_x", "wpot_i", "wpot_m", "wpot_x")
oldnames <- names(data_merge)
data_merge <- data_merge %>% rename_with(~ newnames[which(oldnames == .x)],
.cols = oldnames)
View(data_merge)
# check NULL rows in ArcGIS to make sure they're all coastal points, likely over
# the edge of the environmental raster
null <- data_merge %>% filter(adefi == "NULL")
write.csv(data_merge, "points_all.csv", row.names = F)
View(data_merge)
rm(list = ls())
# bring in both data files
chelsa_data <- read.csv("australia_chelsa_3May2022.csv", header = T, sep = ",", na.strings = "")
setwd("..")
# bring in both data files
chelsa_data <- read.csv("australia_chelsa_3May2022.csv", header = T, sep = ",", na.strings = "")
gbif_data <- read.csv("australia_new_layers_all.csv", header = T, sep = ",", na.strings = "")
# make sure species is a factor and lat/lon are numbers in both datasets
chelsa_data$species <- as.factor(chelsa_data$species)
gbif_data$species <- as.factor(gbif_data$species)
str(chelsa_data)
str(gbif_data)
# execute left join
joined_dat <- left_join(gbif_data, chelsa_data, by = c("species" = "species",
"decimalLatitude" = "decimalLatitude",
"decimalLongitude" = "decimalLongitude"))
# remove duplicates
joined_dat <- joined_dat %>% distinct(species, decimalLongitude,
decimalLatitude, .keep_all = TRUE)
# check out null values that didn't make it through join
nulls <- joined_dat %>% filter(is.na(adefi))
View(chelsa_data)
View(joined_dat)
View(nulls)
getwd()
write.csv(joined_dat, "australia_gbif_chelsa_merge_3May2022.csv", row.names = F)
rm(list = ls())
library(tidyverse)
library(maptools)
library(maps)
library(mapdata)
library(dismo)
library(rgeos)
library(rJava)
library(raster)
library(rgdal)
library(sp)
getwd()
setwd("..")
install.packages("sf")
install.packages("terra")
install.packages("spData")
install.packages("spDataLarge", repos = "https://nowosad.r-universe.dev")
library(tidyverse)
library(ggplot2)
library(ape)
library(ggtree)
install.packages("ggtree")
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("ggtree")
library(ggtree)
sessionInfo()
R.Version()
library(ggplot2) # plotting
library(ggpubr) # plotting
getwd()
setwd("Desktop/01_projects/PBIO_6800_ecological_genomics/ecological_genomics_2023/PopGenomics/results/") # set the path to where you saved the pcANGSD results on your laptop
COV <- as.matrix(read.table("allRS_poly.cov")) # read in the genetic covariance matrix
PCA <- eigen(COV) # extract the principal components from the COV matrix
var <- round(PCA$values/sum(PCA$values),3)
var[1:3]
barplot(var,
xlab="Eigenvalues of the PCA",
ylab="Proportion of variance explained")
names <- read.table("allRS_bam.list")
names <- unlist(strsplit(basename(as.character(names[,1])), split = ".sorted.rmdup.bam"))
split = strsplit(names, "_")
pops <- data.frame(names[1:95], do.call(rbind, split[1:95]))
names(pops) = c("Ind", "Pop", "Row", "Col")
plot(PCA$vectors[,1:2],
col=as.factor(pops[,2]),
xlab="PC1",ylab="PC2",
main="Genetic PCA")
data=as.data.frame(PCA$vectors)
data=data[,c(1:3)]
data= cbind(data, pops)
cols=c("#377eB8","#EE9B00","#0A9396","#94D2BD","#FFCB69","#005f73","#E26D5C","#AE2012", "#6d597a", "#7EA16B","#d4e09b", "gray70")
ggscatter(data, x = "V1", y = "V2",
color = "Pop",
mean.point = TRUE,
star.plot = TRUE) +
theme_bw(base_size = 13, base_family = "Times") +
theme(panel.background = element_blank(),
legend.background = element_blank(),
panel.grid = element_blank(),
plot.background = element_blank(),
legend.text=element_text(size=rel(.7)),
axis.text = element_text(size=13),
legend.position = "bottom") +
labs(x = paste0("PC1: (",var[1]*100,"%)"), y = paste0("PC2: (",var[2]*100,"%)")) +
scale_color_manual(values=c(cols), name="Source population") +
guides(colour = guide_legend(nrow = 2))
q <- read.table("allRS_poly.admix.2.Q", sep=" ", header=F)
K=dim(q)[2] #Find the level of K modeled
## order according to population code
ord<-order(pops[,2])
# make the plot:
barplot(t(q)[,ord],
col=cols[1:K],
space=0,border=NA,
xlab="Populations",ylab="Admixture proportions",
main=paste0("Red spruce K=",K))
