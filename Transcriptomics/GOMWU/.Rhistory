library(tidyverse)
library(CoordinateCleaner) # for post-download cleaning
library(countrycode) # to convert between two and three digit country codes
library(rgbif) # to pull data from GBIF
library(taxize) # to match taxa names to taxon keys
library(purrr)
library(readr)
library(magrittr) # for %T>% pipe
library(bit64) # for viewing certain GBIF columns correctly
getwd*
)
getwd()
setwd("..")
# get the data out of GBIF and onto your computer using download key from meta
(data <- occ_download_get("0261199-210914110416597"))
occurrences <- occ_download_import(data) %>%
dplyr::select(species, decimalLongitude, decimalLatitude, countryCode,
individualCount, gbifID, family, genus, taxonRank,
scientificName, verbatimScientificName,
coordinateUncertaintyInMeters, year, basisOfRecord,
institutionCode, collectionCode, catalogNumber, recordNumber, recordedBy)
setwd("cli")
setwd("clim_tree_taxa")
getwd()
setwd("data/clim_tree_taxa")
avh_cal <- read.csv("calandrinia_AVH_12Apr2022.csv", header = T, sep = ",", na.strings = "")
# select columns to match GBIF data
avh_cal <- avh_cal %>% mutate(gbifID = "Australian Virtual Herbarium") %>%
dplyr::select(species, decimalLongitude, decimalLatitude, countryCode, individualCount,
gbifID, family, genus, taxonRank, scientificName, verbatimScientificName,
coordinateUncertaintyInMeters, year, basisOfRecord, institutionCode,
collectionCode, catalogNumber, recordNumber, recordedBy)
# add AVH data to GBIF data
occurrences$gbifID <- as.character(occurrences$gbifID)
occurrences <- bind_rows(occurrences, avh_cal)
# get rid of records identified only to family level
occurrences <- occurrences %>% filter(taxonRank != "FAMILY" & taxonRank != "family")
# for all records only identified to genus level, record species names as "Genus L."
occurrences$species <- as.factor(occurrences$species)
View(occurrences)
no_sp <- occurrences %>% filter(is.na(species))
View(no_sp)
for (i in 1:nrow(occurrences)) {
if (is.na(occurrences[i,1])) {
occurrences[i,1] <- paste0(occurrences[i,8], " L.")
}
}
str(occurrences)
# for all records only identified to genus level, record species names as "Genus L."
occurrences$species <- as.character(occurrences$species)
for (i in 1:nrow(occurrences)) {
if (is.na(occurrences[i,1])) {
occurrences[i,1] <- paste0(occurrences[i,8], " L.")
}
}
no_sp <- occurrences %>% filter(is.na(species))
no_sp <- occurrences %>% filter(species == "")
for (i in 1:nrow(occurrences)) {
if (occurrences[i,1] == "") {
occurrences[i,1] <- paste0(occurrences[i,8], " L.")
}
}
View(avh_cal)
no_sp <- occurrences %>% filter(species == "")
View(occurrences)
# first make sure columns are all of right type
occurrences$individualCount <- as.numeric(occurrences$individualCount)
occurrences$coordinateUncertaintyInMeters <- as.numeric(occurrences$coordinateUncertaintyInMeters)
occurrences$year <- as.numeric(occurrences$year)
occurrences_clean <- occurrences %>%
filter(!is.na(decimalLongitude)) %>%
filter(!is.na(decimalLatitude)) %>%
filter(coordinateUncertaintyInMeters <= 1000 |
is.na(coordinateUncertaintyInMeters)) %>%
filter(basisOfRecord == "PRESERVED_SPECIMEN") %>%
filter(individualCount > 0 | is.na(individualCount)) %>%
filter(individualCount < 99 | is.na(individualCount))
bad_occ <- setdiff(occurrences, occurrences_clean)
View(bad_occ)
# convert country code from ISO2c to ISO3c (necessary for next step)
# investigate any errors at https://www.iban.com/country-codes
# ZZ = unknown or unspecified country code
occurrences_clean$countryCode <- countrycode(occurrences_clean$countryCode,
origin = "iso2c",
destination = "iso3c",
warn = TRUE, nomatch = NA)
# flag problems
flags <- clean_coordinates(x = occurrences_clean, lon = "decimalLongitude",
lat = "decimalLatitude",
countries = "countryCode",
species = "species",
tests = c("capitals", "centroids", "equal","gbif",
"institutions", "zeros", "countries",
"seas"))
# exclude problematic records after examining them
occurrences_clean <- occurrences_clean[flags$.summary,]
# round lat and lon to 4 decimal places
occurrences_clean$decimalLatitude <- round(occurrences_clean$decimalLatitude, 4)
occurrences_clean$decimalLongitude <- round(occurrences_clean$decimalLongitude, 4)
View(occurrences_clean)
# dedup
occurrences_clean_deduped <- occurrences_clean %>% distinct(species, decimalLongitude,
decimalLatitude, .keep_all = TRUE)
getwd()
setwd("..")
# these will later need to be filtered for rounding coords to a few decimal places, ditching
# duplicates, sampling 1 record per 1 km2, etc.
write.csv(occurrences_clean_deduped, "calandrinia_all_cleaned_3May2022.csv", row.names = F)
rm(list = ls())
getwd()
setwd("parakeelya_chelsa_extract_3May2022/")
# index all files in the directory (this should be a folder containing only the
# files that you want to merge)
file_list <- list.files()
# note: if you run this command more than once, you need to rm(data_merge) or it
# will keep tagging new records onto the end of last run's dataframe
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("data_merge")){
data_merge <- read.csv(file, header = TRUE, sep = "\t")
}
# if the merged dataset does exist, append to it
if (exists("data_merge")){
temp_dataset <- read.csv(file, header = TRUE, sep = "\t") %>%
dplyr::select(RASTERVALU)
data_merge <- cbind(data_merge, temp_dataset)
rm(temp_dataset)
}
}
View(data_merge)
# get rid of a duplicate column (usually the first RASTRERVALU appears twice)
# also remove ArcGIS-generated XCoord and YCoord columns
data_merge <- data_merge[, c(3:5,7:25)]
# clean up column names
newnames <- c("species", "decimalLongitude", "decimalLatitude",
"CHELSA_01", "CHELSA_02", "CHELSA_03", "CHELSA_04", "CHELSA_05", "CHELSA_06",
"CHELSA_07", "CHELSA_08", "CHELSA_09", "CHELSA_10", "CHELSA_11", "CHELSA_12",
"CHELSA_13", "CHELSA_14", "CHELSA_15", "CHELSA_16", "CHELSA_17",
"CHELSA_18", "CHELSA_19")
oldnames <- names(data_merge)
data_merge <- data_merge %>% rename_with(~ newnames[which(oldnames == .x)],
.cols = oldnames)
View(data_merge)
# check NULL rows in ArcGIS to make sure they're all coastal points, likely over
# the edge of the environmental raster
null <- data_merge %>% filter(CHELSA_01 == "NULL")
write.csv(null, "null_points.csv", row.names = F)
# remove NULL rows
data_merge <- data_merge %>% filter(CHELSA_01 != "NULL")
# check NULL rows in ArcGIS to make sure they're all coastal points, likely over
# the edge of the environmental raster
null <- data_merge %>% filter(CHELSA_15 == "NULL")
# put all CHELSA variables as numeric
for (i in 4:22) {
data_merge[,i] <- as.numeric(data_merge[,i])
}
# put temperature variables in units of degrees C
for (i in 4:14) {
data_merge[,i] <- data_merge[,i] * 0.1
}
View(data_merge)
write.csv(data_merge, "points_all.csv", row.names = F)
setwd("..")
write.csv(data_merge, "parakeelya_chelsa_extract_3May2022.csv", row.names = F)
# bring in both data files
chelsa_data <- read.csv("parakeelya_chelsa_extract_3May2022.csv", header = T, sep = ",", na.strings = "")
gbif_data <- read.csv("calandrinia_all_cleaned_3May2022.csv", header = T, sep = ",", na.strings = "")
# make sure species is a factor and lat/lon are numbers in both datasets
chelsa_data$species <- as.factor(chelsa_data$species)
gbif_data$species <- as.factor(gbif_data$species)
# execute left join
joined_dat <- left_join(gbif_data, chelsa_data, by = c("species" = "species",
"decimalLatitude" = "decimalLatitude",
"decimalLongitude" = "decimalLongitude"))
View(joined_dat)
# remove duplicates
joined_dat <- joined_dat %>% distinct(species, decimalLongitude,
decimalLatitude, .keep_all = TRUE)
# check out null values that didn't make it through join
nulls <- joined_dat %>% filter(is.na(CHELSA_15))
View(null)
View(nulls)
6056-2020
# remove points with null values (coastal)
joined_dat <- joined_dat %>% filter(!is.na(CHELSA_01))
write.csv(joined_dat, "parakeelya_gbif_chelsa_merge_3May2022.csv", row.names = F)
# join australian parakeelya and portulaca
port_aus <- read.csv("portulaca_australia_3May2022.txt", header = T, sep = "\t", na.strings = "")
View(port_aus)
port_aus <- port_aus[,c(3:41)]
View(port_aus)
# join australian parakeelya and portulaca
port_aus <- read.csv("portulaca_australia_3May2022.txt", header = T, sep = "\t", na.strings = "")
port_aus <- port_aus[,c(3:40)]
australia <- bind_rows(port_aus, joined_dat)
View(australia)
1971+4036
# check for nulls
nulls <- australia %>% filter(is.na(CHELSA_01))
write.csv("australia_chelsa_3May2022.csv", row.names = F)
write.csv(australia, "australia_chelsa_3May2022.csv", row.names = F)
View(null)
rm(list = ls())
getwd()
setwd("new_aus_layers/")
# index all files in the directory (this should be a folder containing only the
# files that you want to merge)
file_list <- list.files()
# note: if you run this command more than once, you need to rm(data_merge) or it
# will keep tagging new records onto the end of last run's dataframe
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("data_merge")){
data_merge <- read.csv(file, header = TRUE, sep = "\t")
}
# if the merged dataset does exist, append to it
if (exists("data_merge")){
temp_dataset <- read.csv(file, header = TRUE, sep = "\t") %>%
dplyr::select(RASTERVALU)
data_merge <- cbind(data_merge, temp_dataset)
rm(temp_dataset)
}
}
View(data_merge)
View(data_merge)
# get rid of a duplicate column (usually the first RASTRERVALU appears twice)
# also remove ArcGIS-generated XCoord and YCoord columns
data_merge <- data_merge[, c(3:5,7:57)]
# alternatively
newnames <- c("species", "decimalLongitude", "decimalLatitude", "adefi", "adefm", "adefx",
"arid_i", "arid_m", "arid_x", "c4gi", "evapi", "evapm", "evapx", "maxti", "maxtm",
"maxtx", "megagi", "mesogi", "microgi", "minti", "mintm", "mintx", "pwat_i",
"pwat_m", "pwat_x", "radni", "radnm", "radnx", "rhu215_i", "rhu215_m", "rhu215_x",
"rpreci", "rprecx", "rti_i", "rti_x", "rtx_i", "rtx_x", "spls_i", "spls_m",
"spls_x", "srain1", "srain1mp", "srain2", "srain2mp", "trnga", "trngi", "trngm",
"trngx", "wdef_i", "wdef_m", "wdef_x", "wpot_i", "wpot_m", "wpot_x")
oldnames <- names(data_merge)
data_merge <- data_merge %>% rename_with(~ newnames[which(oldnames == .x)],
.cols = oldnames)
View(data_merge)
# check NULL rows in ArcGIS to make sure they're all coastal points, likely over
# the edge of the environmental raster
null <- data_merge %>% filter(adefi == "NULL")
write.csv(data_merge, "points_all.csv", row.names = F)
View(data_merge)
rm(list = ls())
# bring in both data files
chelsa_data <- read.csv("australia_chelsa_3May2022.csv", header = T, sep = ",", na.strings = "")
setwd("..")
# bring in both data files
chelsa_data <- read.csv("australia_chelsa_3May2022.csv", header = T, sep = ",", na.strings = "")
gbif_data <- read.csv("australia_new_layers_all.csv", header = T, sep = ",", na.strings = "")
# make sure species is a factor and lat/lon are numbers in both datasets
chelsa_data$species <- as.factor(chelsa_data$species)
gbif_data$species <- as.factor(gbif_data$species)
str(chelsa_data)
str(gbif_data)
# execute left join
joined_dat <- left_join(gbif_data, chelsa_data, by = c("species" = "species",
"decimalLatitude" = "decimalLatitude",
"decimalLongitude" = "decimalLongitude"))
# remove duplicates
joined_dat <- joined_dat %>% distinct(species, decimalLongitude,
decimalLatitude, .keep_all = TRUE)
# check out null values that didn't make it through join
nulls <- joined_dat %>% filter(is.na(adefi))
View(chelsa_data)
View(joined_dat)
View(nulls)
getwd()
write.csv(joined_dat, "australia_gbif_chelsa_merge_3May2022.csv", row.names = F)
rm(list = ls())
library(tidyverse)
library(maptools)
library(maps)
library(mapdata)
library(dismo)
library(rgeos)
library(rJava)
library(raster)
library(rgdal)
library(sp)
getwd()
setwd("..")
install.packages("sf")
install.packages("terra")
install.packages("spData")
install.packages("spDataLarge", repos = "https://nowosad.r-universe.dev")
library(tidyverse)
library(ggplot2)
library(ape)
library(ggtree)
install.packages("ggtree")
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("ggtree")
library(ggtree)
sessionInfo()
getwd()
setwd("Desktop/01_projects/PBIO_6800_ecological_genomics/tr")
setwd("Desktop/01_projects/PBIO_6800_ecological_genomics/ecological_genomics_2023/Transcriptomics/GOMWU/")
## Import the libraries that we're likely to need in this session
library(DESeq2)
# Try with new counts table from filtered transcriptome assembly
countsTable <- read.table("../myresults/salmon.isoform.counts.matrix.filteredAssembly", header=TRUE, row.names=1)
getwd()
# Try with new counts table from filtered transcriptome assembly
countsTable <- read.table("../myresults/salmon.isoform.counts.matrix.filteredAssembly", header = TRUE, row.names = 1)
head(countsTable)
dim(countsTable)
countsTableRound <- round(countsTable) # bc DESeq2 doesn't like decimals (and Salmon outputs data with decimals)
head(countsTableRound)
#import the sample discription table
conds <- read.delim("../myresults/ahud_samples_R.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1)
head(conds)
dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData = conds,
design = ~ treatment)
dim(dds)
# Filter
dds <- dds[rowSums(counts(dds) >= 30) >= 28,]
nrow(dds)
# Subset the DESeqDataSet to the specific level of the "generation" factor
dds_F0 <- subset(dds, select = generation == 'F0')
dim(dds_F0)
# Perform DESeq2 analysis on the subset
dds_F0 <- DESeq(dds_F0)
resultsNames(dds_F0)
res_F0_OWvAM <- results(dds_F0, name = "treatment_OW_vs_AM", alph a =0.05)
res_F0_OWvAM <- results(dds_F0, name = "treatment_OW_vs_AM", alpha = 0.05)
res_F0_OWvAM <- res_F0_OWvAM[order(res_F0_OWvAM$padj),]
head(res_F0_OWvAM)
summary(res_F0_OWvAM)
res_F0_OWAvAM <- results(dds_F0, name = "treatment_OWA_vs_AM", alpha = 0.05)
res_F0_OWAvAM <- res_F0_OWAvAM[order(res_F0_OWAvAM$padj),]
head(res_F0_OWAvAM)
summary(res_F0_OWAvAM)
res_F0_OAvAM <- results(dds_F0, name = "treatment_OA_vs_AM", alpha = 0.05)
res_F0_OAvAM <- res_F0_OAvAM[order(res_F0_OAvAM$padj),]
head(res_F0_OAvAM)
summary(res_F0_OAvAM)
library(tidyr)
# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OWvAM_df <- data.frame(transcriptID = rownames(res_F0_OWvAM), res_F0_OWvAM)
# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OWvAM_df <- separate(res_F0_OWvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE)
# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OWvAM_df$transcriptID_trim <- paste(res_F0_OWvAM_df$part1, res_F0_OWvAM_df$part2, sep = "::")
# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OWvAM_df <- res_F0_OWvAM_df[, !(names(res_F0_OWvAM_df) %in% c("part1", "part2", "part3", "rest"))]
write.table(res_F0_OWvAM_df, file = "res_F0_OWvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records
# Select the two columns we want to save for the GOMWU analysis
selected_columns_OW <- res_F0_OWvAM_df[c("transcriptID_trim", "log2FoldChange")]
# Save the selected columns as a CSV file
write.csv(selected_columns_OW, file = "res_F0_OWvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU
# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OWAvAM_df <- data.frame(transcriptID = rownames(res_F0_OWAvAM), res_F0_OWAvAM)
# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OWAvAM_df <- separate(res_F0_OWAvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE)
# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OWAvAM_df$transcriptID_trim <- paste(res_F0_OWAvAM_df$part1, res_F0_OWAvAM_df$part2, sep = "::")
# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OWAvAM_df <- res_F0_OWAvAM_df[, !(names(res_F0_OWAvAM_df) %in% c("part1", "part2", "part3", "rest"))]
write.table(res_F0_OWAvAM_df, file = "res_F0_OWAvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records
# Select the two columns we want to save for the GOMWU analysis
selected_columns_OWA <- res_F0_OWAvAM_df[c("transcriptID_trim", "log2FoldChange")]
# Save the selected columns as a CSV file
write.csv(selected_columns_OWA, file = "res_F0_OWAvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU
# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OAvAM_df <- data.frame(transcriptID = rownames(res_F0_OAvAM), res_F0_OAvAM)
# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OAvAM_df <- separate(res_F0_OAvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE)
# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OAvAM_df$transcriptID_trim <- paste(res_F0_OAvAM_df$part1, res_F0_OAvAM_df$part2, sep = "::")
# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OAvAM_df <- res_F0_OAvAM_df[, !(names(res_F0_OAvAM_df) %in% c("part1", "part2", "part3", "rest"))]
write.table(res_F0_OAvAM_df, file = "res_F0_OAvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records
# Select the two columns we want to save for the GOMWU analysis
selected_columns_OA <- res_F0_OAvAM_df[c("transcriptID_trim", "log2FoldChange")]
# Save the selected columns as a CSV file
write.csv(selected_columns_OA, file = "res_F0_OAvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU
View(res_F0_OAvAM_df)
# Edit these to match your data file names:
input = "res_F0_OWvAM_LFC.csv" # two columns of comma-separated values: gene id, continuous measure of significance. To perform standard GO enrichment analysis based on Fisher's exact test, use binary measure (0 or 1, i.e., either sgnificant or not).
goAnnotations = "trinotate_annotation_GOblastx_onlyanns_onlyGOs_justmergeGeneIDtab34.txt" # two-column, tab-delimited, one line per gene, multiple GO terms separated by semicolon. If you have multiple lines per gene, use nrify_GOtable.pl prior to running this script.
goDatabase = "go.obo" # download from http://www.geneontology.org/GO.downloads.ontology.shtml
goDivision = "BP" # either MF, or BP, or CC
source("gomwu.functions.R")
gomwuStats(input, goDatabase, goAnnotations, goDivision,
perlPath = "perl", # replace with full path to perl executable if it is not in your system's PATH already
largest = 0.05,  # a GO category will not be considered if it contains more than this fraction of the total number of genes
smallest = 10,   # a GO category should contain at least this many genes to be considered
clusterCutHeight = 0.25, # threshold for merging similar (gene-sharing) terms. See README for details.
#	Alternative="g" # by default the MWU test is two-tailed; specify "g" or "l" of you want to test for "greater" or "less" instead.
#	Module=TRUE,Alternative="g" # un-remark this if you are analyzing a SIGNED WGCNA module (values: 0 for not in module genes, kME for in-module genes). In the call to gomwuPlot below, specify absValue=0.001 (count number of "good genes" that fall into the module)
#	Module=TRUE # un-remark this if you are analyzing an UNSIGNED WGCNA module
)
gomwuStats(input, goDatabase, goAnnotations, goDivision,
perlPath = "perl", # replace with full path to perl executable if it is not in your system's PATH already
largest = 0.05,  # a GO category will not be considered if it contains more than this fraction of the total number of genes
smallest = 10,   # a GO category should contain at least this many genes to be considered
clusterCutHeight = 0.25 # threshold for merging similar (gene-sharing) terms. See README for details.
#	Alternative="g" # by default the MWU test is two-tailed; specify "g" or "l" of you want to test for "greater" or "less" instead.
#	Module=TRUE,Alternative="g" # un-remark this if you are analyzing a SIGNED WGCNA module (values: 0 for not in module genes, kME for in-module genes). In the call to gomwuPlot below, specify absValue=0.001 (count number of "good genes" that fall into the module)
#	Module=TRUE # un-remark this if you are analyzing an UNSIGNED WGCNA module
)
